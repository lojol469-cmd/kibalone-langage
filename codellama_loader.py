#!/usr/bin/env python3
"""
Chargeur optimis√© pour Code Llama 7B avec quantification 4-bit
R√©duit l'usage m√©moire de ~14GB √† ~3.5GB
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from pathlib import Path
import json
import time

class OptimizedCodeLlamaLoader:
    """Chargeur optimis√© pour Code Llama avec quantification"""

    def __init__(self, model_path="./ia/codellama-7b"):
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.is_loaded = False

    def load_model(self):
        """Charge le mod√®le avec quantification 4-bit"""
        print("üöÄ Chargement de Code Llama 7B avec quantification 4-bit...")

        if not self.model_path.exists():
            raise FileNotFoundError(f"Mod√®le non trouv√©: {self.model_path}")

        # Configuration de quantification 4-bit
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

        try:
            # Charger le tokenizer
            print("üìù Chargement du tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            # Charger le mod√®le avec quantification
            print("ü§ñ Chargement du mod√®le (quantification 4-bit)...")
            start_time = time.time()

            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                quantization_config=quantization_config,
                device_map="auto",  # Utilise automatiquement GPU/CPU
                trust_remote_code=True,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )

            load_time = time.time() - start_time
            self.is_loaded = True

            print(f"‚úÖ Mod√®le charg√© en {load_time:.1f} secondes")
            print(f"üíæ M√©moire GPU utilis√©e: {self.get_gpu_memory_usage()}")

            return True

        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {e}")
            return False

    def get_gpu_memory_usage(self):
        """Retourne l'usage m√©moire GPU"""
        if torch.cuda.is_available():
            return f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB"
        return "N/A (pas de GPU)"

    def generate_code(self, prompt, max_length=512, temperature=0.7):
        """G√©n√®re du code √† partir d'un prompt"""
        if not self.is_loaded:
            return "Erreur: mod√®le non charg√©"

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    num_return_sequences=1,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response

        except Exception as e:
            return f"Erreur g√©n√©ration: {e}"

    def analyze_and_modify_parameters(self, cell_type, current_params, environmental_data):
        """Analyse et propose des modifications de param√®tres pour une cellule"""

        prompt = f"""[INST] Analyse cette situation pour une cellule {cell_type} et retourne seulement du JSON.

Param√®tres actuels: {current_params}
Environnement: {environmental_data}

Retourne exactement ce format JSON:
{{
    "internal_states": {{"photosynth√®se_rate": 1.2, "r√©sistance_stress": 0.8}},
    "physical_objects": {{"feuilles": {{"efficacit√©": 0.95}}}},
    "reasoning": "Explication de l'adaptation"
}}

Pas d'autre texte, juste le JSON. [/INST]"""

        response = self.generate_code(prompt, max_length=512, temperature=0.1)  # Tr√®s basse temp√©rature pour plus de pr√©cision

        try:
            # Nettoyer la r√©ponse
            response = response.strip()

            # Supprimer les balises si pr√©sentes
            if "[/INST]" in response:
                response = response.split("[/INST]")[-1].strip()

            # Chercher le JSON
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1

            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                modifications = json.loads(json_str)
                return modifications

        except Exception as e:
            return {"error": f"Erreur parsing: {str(e)[:100]}", "raw_response": response[:300]}

        return {"error": "Aucun JSON trouv√©", "raw_response": response[:300]}

def test_codellama_integration():
    """Test l'int√©gration de Code Llama dans le syst√®me cellulaire"""

    print("üß™ Test d'int√©gration Code Llama pour cellules autonomes\n")

    # Initialiser le chargeur
    loader = OptimizedCodeLlamaLoader()

    # Charger le mod√®le
    if not loader.load_model():
        print("‚ùå √âchec du chargement du mod√®le")
        return

    # Test avec des donn√©es d'exemple pour un arbre
    test_params = {
        "photosynth√®se_rate": 1.0,
        "absorption_eau": 0.8,
        "r√©sistance_stress": 0.6
    }

    test_environment = {
        "temperature": 30.0,
        "light_level": 85.0,
        "soil_moisture": 35.0,
        "wind_speed": 8.0
    }

    print("üå≥ Test d'adaptation pour un Arbre:")
    print(f"Param√®tres actuels: {test_params}")
    print(f"Environnement: {test_environment}\n")

    # G√©n√©rer des adaptations
    adaptations = loader.analyze_and_modify_parameters("Arbre", test_params, test_environment)

    print("üîÑ Adaptations propos√©es:")
    print(json.dumps(adaptations, indent=2))

if __name__ == "__main__":
    test_codellama_integration()