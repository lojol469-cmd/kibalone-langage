#!/usr/bin/env python3
"""
T√©l√©chargeur de mod√®les LLM pour cellules autonomes
Permet de choisir entre diff√©rents mod√®les selon les besoins
"""

import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def download_phi_15():
    """T√©l√©charge Phi-1.5 (1.3GB) - mod√®le actuel"""
    print("üì• T√©l√©chargement de Microsoft Phi-1.5 (1.3GB)...")
    model_path = Path("./ia/phi-1_5")

    if model_path.exists():
        print("‚úÖ Phi-1.5 d√©j√† pr√©sent")
        return str(model_path)

    try:
        tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")
        model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5")
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)
        print("‚úÖ Phi-1.5 t√©l√©charg√© avec succ√®s")
        return str(model_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement de Phi-1.5: {e}")
        return None

def download_phi_2():
    """T√©l√©charge Phi-2 (2.7GB) - mod√®le plus puissant pour g√©n√©ration de code"""
    print("üì• T√©l√©chargement de Microsoft Phi-2 (2.7GB)...")
    model_path = Path("./ia/phi-2")

    if model_path.exists():
        print("‚úÖ Phi-2 d√©j√† pr√©sent")
        return str(model_path)

    try:
        snapshot_download(
            repo_id="microsoft/phi-2",
            local_dir=str(model_path),
            local_dir_use_symlinks=False
        )
        print("‚úÖ Phi-2 t√©l√©charg√© avec succ√®s")
        return str(model_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement de Phi-2: {e}")
        return None

def download_starcoder_1b():
    """T√©l√©charge StarCoder-1B (environ 1.8GB) - mod√®le de code optimis√©"""
    print("üì• T√©l√©chargement de StarCoder-1B (1.8GB)...")
    model_path = Path("./ia/starcoder-1b")

    if model_path.exists():
        print("‚úÖ StarCoder-1B d√©j√† pr√©sent")
        return str(model_path)

    try:
        snapshot_download(
            repo_id="bigcode/starcoder",
            local_dir=str(model_path),
            local_dir_use_symlinks=False
        )
        print("‚úÖ StarCoder-1B t√©l√©charg√© avec succ√®s")
        return str(model_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement de StarCoder-1B: {e}")
        return None

def download_deepseek_coder_1b():
    """T√©l√©charge DeepSeek-Coder-1.3B (environ 2.6GB)"""
    print("üì• T√©l√©chargement de DeepSeek-Coder-1.3B (2.6GB)...")
    model_path = Path("./ia/deepseek-coder-1b")

    if model_path.exists():
        print("‚úÖ DeepSeek-Coder-1.3B d√©j√† pr√©sent")
        return str(model_path)

    try:
        snapshot_download(
            repo_id="deepseek-ai/deepseek-coder-1.3b-base",
            local_dir=str(model_path),
            local_dir_use_symlinks=False
        )
        print("‚úÖ DeepSeek-Coder-1.3B t√©l√©charg√© avec succ√®s")
        return str(model_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement de DeepSeek-Coder-1.3B: {e}")
        return None

def download_codellama_7b():
    """T√©l√©charge CodeLlama-7B-Instruct (environ 14GB)"""
    print("üì• T√©l√©chargement de CodeLlama-7B-Instruct (14GB)...")
    model_path = Path("./ia/codellama-7b")

    if model_path.exists():
        print("‚úÖ CodeLlama-7B d√©j√† pr√©sent")
        return str(model_path)

    try:
        snapshot_download(
            repo_id="codellama/CodeLlama-7b-Instruct-hf",
            local_dir=str(model_path),
            local_dir_use_symlinks=False
        )
        print("‚úÖ CodeLlama-7B t√©l√©charg√© avec succ√®s")
        return str(model_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du t√©l√©chargement de CodeLlama-7B: {e}")
        return None

def test_model_loading(model_path, model_name):
    """Teste le chargement du mod√®le"""
    print(f"üß™ Test du chargement de {model_name}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path)

        # Test simple de g√©n√©ration
        test_prompt = "def modify_parameter(value, factor):"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=20, temperature=0.7)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print(f"‚úÖ {model_name} charg√© et test√© avec succ√®s")
        print(f"   Test g√©n√©ration: {response[len(test_prompt):].strip()[:50]}...")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test de {model_name}: {e}")
        return False

def main():
    print("ü§ñ T√©l√©chargeur de mod√®les LLM pour cellules autonomes")
    print("=" * 60)

    models = {
        "1": ("Phi-1.5 (actuel)", download_phi_15, "~1.3GB"),
        "2": ("Phi-2 (recommand√©)", download_phi_2, "~2.7GB"),
        "3": ("StarCoder-1B", download_starcoder_1b, "~1.8GB"),
        "4": ("DeepSeek-Coder-1.3B", download_deepseek_coder_1b, "~2.6GB"),
        "5": ("CodeLlama-7B (puissant)", download_codellama_7b, "~14GB")
    }

    print("Mod√®les disponibles pour modification autonome de param√®tres:")
    for key, (name, func, size) in models.items():
        print(f"  {key}. {name} ({size}) - {getattr(func, '__doc__', '').split('-')[0].strip()}")

    choice = input("\nChoisissez un mod√®le (1-5) ou 'q' pour quitter: ").strip()

    if choice.lower() == 'q':
        return

    if choice not in models:
        print("‚ùå Choix invalide")
        return

    model_name, download_func, size = models[choice]
    print(f"\nüì• T√©l√©chargement de {model_name} ({size})...")
    print("‚ö†Ô∏è  Cela peut prendre du temps selon votre connexion internet")
    print("üí° Le mod√®le permettra aux cellules de modifier leurs param√®tres en temps r√©el")

    model_path = download_func()

    if model_path and test_model_loading(model_path, model_name):
        print(f"\n‚úÖ {model_name} pr√™t √† √™tre utilis√© par les cellules autonomes!")
        print(f"   Chemin: {model_path}")
        print("\nüîß Pour utiliser ce mod√®le, modifiez ecosystem_simulation.py:")
        print("   Dans AutonomousBrain.__init__(), changez le model_path")
        print("   pour pointer vers ce nouveau mod√®le.")
    else:
        print(f"\n‚ùå √âchec du t√©l√©chargement/test de {model_name}")

if __name__ == "__main__":
    main()